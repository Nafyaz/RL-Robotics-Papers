1. Machado et al. “Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents” Journal of Artificial Intelligence Research (2018) URL: https://jair.org/index.php/jair/article/view/11182
## Generated by https://www.findmypapers.ai

1. Foundation Models and Large-Scale Learning:

    A major thrust in recent research involves integrating foundation models (FMs), including Large Language Models (LLMs) and Vision-Language Models (VLMs), into robotic systems [1] [2]. These models provide powerful capabilities for understanding natural language instructions, reasoning about tasks, and enabling zero-shot or few-shot generalization [3] [4].
    Papers like GR00T N1 introduce open foundation models specifically for humanoid robots, using a dual-system architecture (vision-language + diffusion transformer) trained on diverse data including real robot trajectories, human videos, and synthetic datasets [5].
    Frameworks like COME-robot utilize GPT-4V for closed-loop reasoning and adaptive planning in real-world mobile manipulation tasks [6]. STEER focuses on training language-grounded policies with dense annotation to bridge high-level reasoning and low-level control [7].
    The development of large-scale datasets and benchmarks is crucial. AgiBot World introduces a platform with over 1 million trajectories across numerous tasks and proposes a generalist policy (GO-1) showing scalable performance [8]. The Open X-Embodiment dataset and RT-X models also represent efforts in this direction [9].
    UniAct proposes a framework using a tokenized "Universal Action Space" to handle heterogeneity in action spaces across different robots, enhancing cross-domain data utilization and generalization [10].
    Heterogeneous Pre-trained Transformers (HPT) investigate pre-training large policy networks on data from diverse embodiments and tasks to create general, shareable representations [11].

2. Advanced Reinforcement Learning and Imitation Learning:

    Imitation Learning (IL): IL, especially from human demonstrations, remains a core technique. Recent work focuses on scaling IL with large datasets [12], improving generalization [13], and making it more data-efficient [14].
        Regularized Optimal Transport (ROT) combines trajectory-matching rewards with behavior cloning for faster imitation [15].
        Reasoning through Action-free Data (RAD) leverages language-based reasoning from action-free human videos to train generalizable policies [13].
        NIL proposes learning motor skills from 2D videos generated by diffusion models, bypassing the need for expert 3D demonstrations [16].
    Reinforcement Learning (RL): Deep RL (DRL) continues to be applied for complex control tasks, including locomotion and manipulation [17] [18].
        Challenges in real-world RL, such as sample efficiency and reward design, are actively being addressed [19] [20].
        FLaRe proposes a large-scale RL fine-tuning framework for pre-trained policies, achieving SOTA performance on mobile manipulation tasks [21].
        Self-Improving Robots: MEDAL++ aims for robots that learn autonomously by learning to both do and undo tasks, inferring rewards from initial demonstrations [22]. SOAR uses foundation models for autonomous data collection and self-supervised improvement [23].
        Offline RL and data-driven methods are gaining traction, using pre-collected datasets [20].
        Diffusion models are being integrated into policies, such as Diffusion Policy [24] and DiT-Block Policy [25].
    Combining IL and RL: Many approaches combine IL for pre-training or bootstrapping with RL for fine-tuning or improving robustness [21].

3. Generalization, Transferability, and Data Augmentation:

    A key goal is developing policies that generalize to new tasks, objects, and environments [26] [27].
    Techniques include multi-task learning [26], learning from diverse data [11], and using robust representations [10].
    Data augmentation is critical. Generative Augmentation uses semantic scene augmentations to expand small real-world datasets [28]. EAGLE proposes control-aware augmentation to avoid disrupting task-relevant information [29]. HybridGen uses VLMs and hybrid planning to generate diverse demonstration data [30].

4. Specific Applications and Benchmarks:

    Manipulation: Dexterous manipulation [26], vision-based manipulation [31] [32], bimanual manipulation [33], and long-horizon tasks [34] are common focuses. Benchmarks like HumanoidBench [35], ARNOLD [36], and Mimicking-Bench [37] are being developed.
    Locomotion & Navigation: Learning locomotion for humanoids [35], mobile robot navigation [38], and vision-and-language navigation [39] are explored.
    Human-Robot Interaction: RL is used in social robotics [40] [40] and for learning from human preferences or feedback [41] [42].

5. Surveys and Frameworks:

    Several recent surveys provide overviews of RL in robotics [17] [18] [43], deep RL for manipulation [43], and the integration of foundation models [1] [2].
    Frameworks like PIC4rl-gym [38] and Robel [19] provide tools for research.

In summary, the state-of-the-art involves scaling learning using large, diverse datasets and powerful foundation models, combined with sophisticated RL and IL algorithms, aiming for generalist robots capable of complex tasks in real-world environments. The focus is shifting from task-specific specialists to adaptable, general-purpose agents.
Referenced Papers (43)

[1] A Survey on Robotics with Foundation Models: toward Embodied AI
    arXiv:2402.02385v1

[2] Integrating Reinforcement Learning with Foundation Models for Autonomous Robotics: Methods and Perspectives
arXiv:2410.16411v1

[3]Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents
arXiv:2311.13373v6
[4]Large Language Models Humanize Technology
arXiv:2305.05576v1
[5]GR00T N1: An Open Foundation Model for Generalist Humanoid Robots
arXiv:2503.14734v1
[6]Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V
arXiv:2404.10220v1
[7]STEER: Flexible Robotic Manipulation via Dense Language Grounding
arXiv:2411.03409v1
[8]AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems
arXiv:2503.06669v2
[9]From Mystery to Mastery: Failure Diagnosis for Improving Manipulation Policies
arXiv:2412.02818v1
[10]Universal Actions for Enhanced Embodied Foundation Models
arXiv:2501.10105v1
[11]Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers
arXiv:2409.20537v1
[12]Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale
arXiv:2204.03514v2
[13]Action-Free Reasoning for Policy Generalization
arXiv:2502.03729v2
[14]Learning from Demonstration with Implicit Nonlinear Dynamics Models
arXiv:2409.18768v3
[15]Watch and Match: Supercharging Imitation with Regularized Optimal Transport
arXiv:2206.15469v2
[16]NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models
arXiv:2503.10626v1
[17]A Concise Introduction to Reinforcement Learning in Robotics
arXiv:2210.07397v1
[18]Forward and inverse reinforcement learning sharing network weights and hyperparameters
arXiv:2008.07284v2
[19]Benchmarking Reinforcement Learning Methods for Dexterous Robotic Manipulation with a Three-Fingered Gripper
arXiv:2408.14747v1
[20]OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning
arXiv:2010.13611v3
[21]FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning
arXiv:2409.16578v2
[22]Self-Improving Robots: End-to-End Autonomous Visuomotor Reinforcement Learning
arXiv:2303.01488v1
[23]Autonomous Improvement of Instruction Following Skills via Foundation Models
arXiv:2407.20635v2
[24]TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning
arXiv:2412.10447v1
[25]The Ingredients for Robotic Diffusion Transformers
arXiv:2410.10088v1
[26]Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning
arXiv:2111.03062v1
[27]Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments
arXiv:2409.05865v1
[28]Semantically Controllable Augmentations for Generalizable Robot Learning
arXiv:2409.00951v1
[29]Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation
arXiv:2401.09258v2
[30]HybridGen: VLM-Guided Hybrid Planning for Scalable Data Generation of Imitation Learning
arXiv:2503.13171v1
[31]Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments
arXiv:2010.11940v1
[32]Residual Policy Learning
arXiv:1812.06298v2
[33]Towards More Sample Efficiency in Reinforcement Learning with Data Augmentation
arXiv:1910.09959v3
[34]Single-Shot Learning of Stable Dynamical Systems for Long-Horizon Manipulation Tasks
arXiv:2410.01033v2
[35]TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control
arXiv:2502.17322v1
[36]ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes
arXiv:2304.04321v2
[37]Mimicking-Bench: A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicking
arXiv:2412.17730v1
[38]PIC4rl-gym: a ROS2 modular framework for Robots Autonomous Navigation with Deep Reinforcement Learning
arXiv:2211.10714v1
[39]A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning
arXiv:2210.03112v3
[40]Reinforcement Learning Approaches in Social Robotics
arXiv:2009.09689v4
[41]Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models
arXiv:2311.02379v1
[42]Active Preference-Based Gaussian Process Regression for Reward Learning
arXiv:2005.02575v2
[43]Automated acquisition of structured, semantic models of manipulation activities from human VR demonstration
arXiv:2011.13689v1
